{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my second attempt at Housing Prices. I got pretty much lost in the first one but I learnt quite a few things. Even though I had poor results, it was good for skillup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic libraries first\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loadint data\n",
    "data=pd.read_csv(\"../Housing Prices/train.csv\")\n",
    "testdata=pd.read_csv(\"../Housing Prices/test.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nas for plot- just the train data. not part of final solution\n",
    "#lets fill nans\n",
    "# data.isna().sum()\n",
    "cont_features=[]\n",
    "cat_features=[]\n",
    "data.drop('Id',axis=1,inplace=True)\n",
    "data.GarageType.fillna('None')\n",
    "for col in data.columns:\n",
    "    if data[col].dtype=='int64' or data[col].dtype=='float64':\n",
    "        cont_features.append(col)\n",
    "        data[col].fillna(data[col].median(),inplace=True)\n",
    "    else:\n",
    "        cat_features.append(col)\n",
    "        data[col].fillna(data[col].value_counts().idxmax(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots I used extensively for feature engineering, feature selection\n",
    "# a = sns.FacetGrid(data=data,col=cont_features)\n",
    "for col in cont_features:\n",
    "    fig,ax=plt.subplots(figsize=(5,5))\n",
    "    sns.regplot(x=col,y='SalePrice', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots I used extensively for feature engineering, feature selection\n",
    "for col in cat_features:\n",
    "#     fig,ax=plt.subplots(figsize=(5,5))\n",
    "    sns.catplot(x=col,y='SalePrice', data=data,kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map I used extensively for feature engineering, feature selection\n",
    "fig, ax=plt.subplots(figsize=(40,40))\n",
    "sns.heatmap(data.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I got these using visual analysis of data\n",
    "cont_features2=['SalePrice','LotFrontage','MasVnrArea', 'TotalBsmtSF', '1stFlrSF','2ndFlrSF', 'GrLivArea'] \n",
    "cat_features2= ['YearBuilt','YearRemodAdd','BsmtFullBath', 'BsmtHalfBath','FullBath','HalfBath','OverallQual','TotRmsAbvGrd', 'Fireplaces','MSZoning','Street','LotShape','LotConfig','Condition1',\n",
    "        'BldgType','Neighborhood','HouseStyle',\n",
    "        'BsmtFinType1','Electrical','FireplaceQu','GarageType','SaleType','SaleCondition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewdetfix(feat):\n",
    "    sk=feat.skew()\n",
    "    if sk>1 or sk<1:\n",
    "#         fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "#         feat.fillna(0)\n",
    "#         sns.distplot(feat, ax=ax1)\n",
    "        feat=np.log1p(feat)\n",
    "#         feat.fillna(0)\n",
    "#         sns.distplot(feat,ax=ax2,color=\"k\")\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import percentile\n",
    "# print(data.GrLivArea.shape)\n",
    "\n",
    "\n",
    "def outlierdetectfix(feat):\n",
    "#     print(feat)\n",
    "    q25,q75=percentile(feat,25),percentile(feat,75)\n",
    "    iqr=(q75-q25)\n",
    "    iqr = q75 - q25\n",
    "#     type(feat)\n",
    "    lower = q25 - (iqr*1.5)\n",
    "    upper = q75 + (iqr*1.5)\n",
    "#     print(upper)\n",
    "    feat.loc[feat>upper] = upper\n",
    "    return feat\n",
    "\n",
    "\n",
    "# data.GrLivArea=oulierdetectfix(data.GrLivArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/numpy/lib/function_base.py:4291: RuntimeWarning: Invalid value encountered in percentile\n",
      "  interpolation=interpolation)\n"
     ]
    }
   ],
   "source": [
    "for col in cont_features2:\n",
    "    data[col]=skewdetfix(data[col])\n",
    "    data[col]=outlierdetectfix(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 80)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testdata['SalePrice']=1\n",
    "y=data[['SalePrice']].copy()\n",
    "data.drop('SalePrice',axis=1,inplace=True)\n",
    "data2=pd.concat([data,testdata],axis=0)\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nas\n",
    "data2.GarageType.fillna('None')\n",
    "for col in data2.columns:\n",
    "    if data2[col].dtype=='int64' or data2[col].dtype=='float64':\n",
    "#         cont_features.append(col)\n",
    "        data2[col].fillna(data2[col].median(),inplace=True)\n",
    "    else:\n",
    "#         cat_features.append(col)\n",
    "        data2[col].fillna(data2[col].value_counts().idxmax(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop extra cols\n",
    "for col in data2.columns:\n",
    "#     print (col)\n",
    "    if col not in cont_features2 and col not in cat_features2:\n",
    "        data2.drop(col, axis=1, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "data2['Make']=data2[['YearBuilt','YearRemodAdd']].max(axis=1)\n",
    "data2.loc[ data2['Make'] <= 1950, 'Make'] = 1950\n",
    "data2.loc[(data2['Make'] > 1950) & (data2['Make']<= 1960), 'Make'] = 1960\n",
    "data2.loc[(data2['Make'] > 1960) & (data2['Make']<= 1970), 'Make'] = 1970\n",
    "data2.loc[(data2['Make'] > 1970) & (data2['Make']<= 1980), 'Make'] = 1980\n",
    "data2.loc[(data2['Make'] > 1980) & (data2['Make']<= 1990), 'Make'] = 1990\n",
    "data2.loc[(data2['Make'] > 1990) & (data2['Make']<= 2000), 'Make'] = 2000\n",
    "data2.loc[(data2['Make'] > 2000) & (data2['Make']<= 2010), 'Make'] = 2010\n",
    "data2.loc[ data2['Make'] > 2010, 'Make'] = 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data2.drop(['YearBuilt','YearRemodAdd'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['TotalFSF']=data2['1stFlrSF']+data2['2ndFlrSF']\n",
    "data2.drop(['1stFlrSF','2ndFlrSF'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['TotalBath']=data2['BsmtFullBath']+data2['BsmtHalfBath']+data2['FullBath']+data2['HalfBath']\n",
    "data2.drop(['BsmtFullBath', 'BsmtHalfBath','FullBath','HalfBath'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used plots to get an idea of the relationships and the variables that seem necessary. I fixed the outliers in training data before anything else so that I can combine data and then do missing values, feature engineering and encoding on all the data together. Since the data isnt neccesarily ordinal, I will use onehot encode and then do PCA of the whole set together. That way I can try out different models without worrying about cat features. I will do a correlation check before to check if I am taking the right features. I might do another corr after encoding. lets see how this goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.LotShape.replace(['IR2','IR3'], 'other', inplace=True)\n",
    "data2.Condition1.replace(['Feedr','Artery','RRAn','PosN','RRAe','PosA','RRNn','RRNe'],'other',inplace=True)\n",
    "\n",
    "data2.Electrical.replace(['FuseA','FuseF','FuseP'],'other',inplace=True)\n",
    "\n",
    "data2.GarageType.replace(['BuiltIn','Basment','CarPort','2Types'],'other',inplace=True)\n",
    "data2.SaleType.replace(['COD','ConLD','ConLw','ConLi','CWD','Oth','Con'],'other',inplace=True)\n",
    "\n",
    "data2.SaleCondition.replace(['Family','Alloca','AdjLand'],'other',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#encoding and scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.drop('Neighborhood',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cont_features3=['LotFrontage', 'MasVnrArea', 'TotalBsmtSF', 'TotalFSF', 'GrLivArea'] \n",
    "cat_features3=[ 'Make','TotalBath','OverallQual','TotRmsAbvGrd', 'Fireplaces','MSZoning','Street','LotShape','LotConfig','Condition1',\n",
    "        'BldgType','HouseStyle',\n",
    "        'BsmtFinType1','Electrical','FireplaceQu','GarageType','SaleType','SaleCondition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(var):\n",
    "    return var.astype(\"category\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_features3:\n",
    "    data2[col]=encoder(data2[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data2[cat_features3])\n",
    "# dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2.drop(cat_features3,axis=1,inplace=True)\n",
    "data2=pd.concat([data2,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.drop(cat_features3,axis=1,inplace=True)\n",
    "# data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data2.iloc[:1460,:]\n",
    "test=data2.iloc[1460 : ,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Instantiate encoder/scaler\n",
    "scaler = MinMaxScaler([0,1])\n",
    "scaler.fit(train[cont_features3]) \n",
    "train[cont_features3]=scaler.transform(train[cont_features3]) \n",
    "test[cont_features3]=scaler.transform(test[cont_features3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.93 19.34 25.79 30.71 34.97 38.66 41.91 45.03 48.06 50.97 53.75 56.38\n",
      " 58.95 61.28 63.46 65.49 67.41 69.15 70.84 72.35 73.83 75.21 76.58 77.86\n",
      " 79.07 80.26 81.38 82.47 83.51 84.48 85.43 86.34 87.2  88.04 88.85 89.6\n",
      " 90.34 91.04 91.72 92.36 92.94 93.48 94.   94.47 94.92 95.34 95.75 96.15\n",
      " 96.53 96.87 97.17 97.45 97.7  97.94 98.14 98.33 98.5  98.66 98.81 98.96]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "# X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=60)\n",
    "train2=pca.fit_transform(train)\n",
    "\n",
    "\n",
    "var=pca.explained_variance_ratio_\n",
    "var1=np.cumsum(np.round(var,decimals=4)*100)\n",
    "\n",
    "print(var1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "train_PCA=pca.fit(train)\n",
    "train_PCA=pca.transform(train)\n",
    "test_PCA=pca.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y.iloc[:1460,:]\n",
    "y_test=data2.iloc[1460 : ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_trn,y_tst=train_test_split(train_PCA,y_train,test_size=0.33,random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71230103,  0.91124651,  0.61100757, ...,  0.04207268,\n",
       "         0.09609671,  0.12164085],\n",
       "       [ 0.21947424, -0.24042419, -0.75601532, ...,  0.29149439,\n",
       "         0.15922704, -0.15188291],\n",
       "       [-0.80775444, -0.76163501,  0.21082279, ...,  0.03601982,\n",
       "         0.05428591, -0.03309107],\n",
       "       ...,\n",
       "       [ 0.01385213,  0.35790724,  0.77899443, ..., -0.0650179 ,\n",
       "        -0.12424245, -0.01888905],\n",
       "       [ 1.57625333,  0.8296774 , -0.81030444, ..., -0.33174517,\n",
       "        -0.09866577, -0.0391767 ],\n",
       "       [-0.20478006,  0.40306654,  1.12122459, ...,  0.28574781,\n",
       "        -0.24367592, -0.5074157 ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomforest test\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# rf2.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2=RandomForestClassifier(n_estimators= 3000,min_samples_split= 5,min_samples_leaf= 2)\n",
    "# rf2.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-d4a78521fd95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/environments/PFDS/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_y_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mDOUBLE\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/PFDS/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_y_class_weight\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_y_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/PFDS/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "rf2.fit(x_train,y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_columns_idx = [data2.columns.get_loc(col) \n",
    "#                    for col in cat_features3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe    = OneHotEncoder(categorical_features=cat_columns_idx,sparse=False,handle_unknown='ignore',dtype='int')\n",
    "# # #  for col in cat_features3:\n",
    "# dummies=    ohe.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# lb = LabelBinarizer()\n",
    "# for col in cat_features3:\n",
    "#     feature=lb.fit_transform(data2[col])\n",
    "#     n_feature=pd.concat([n_feature,pd.DataFrame(feature)\n",
    "# #     data3=pd.concat([data2,n_feature],axis=1)\n",
    "#     print(n_feature.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[cat_features3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumm=pd.DataFrame(dummies)\n",
    "\n",
    "dumm.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def skewdetfix(feat):\n",
    "#     sk=feat.skew()\n",
    "#     if sk>1 or sk<1:\n",
    "#         feat=np.log(feat)\n",
    "#     return feat\n",
    "# # data.SalePrice.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import skew\n",
    "# skew(data.SalePrice)\n",
    "# data.SalePrice=np.log(data.SalePrice)\n",
    "# data.SalePrice.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just checking relation between garrage building and others \n",
    "# sns.relplot(x=\"GarageYrBlt\",y=\"YearRemodAdd\",data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #function for outliers on train data\n",
    "# #  from scipy.stats import iqr\n",
    "# # def outlierremove(ser)\n",
    "# from numpy import percentile\n",
    "# # print(data.GrLivArea.shape)\n",
    "\n",
    "\n",
    "# def oulierdetectfix(feat):\n",
    "# #     print(feat)\n",
    "#     q25,q75=percentile(feat,25),percentile(feat,75)\n",
    "#     iqr=(q75-q25)\n",
    "#     iqr = q75 - q25\n",
    "# #     type(feat)\n",
    "#     lower = q25 - (iqr*3)\n",
    "#     upper = q75 + (iqr*3)\n",
    "# #     print(upper)\n",
    "#     feat.loc[feat>upper] = upper\n",
    "# #     return feata\n",
    "# data.GrLivArea=oulierdetectfix(data.GrLivArea)\n",
    "# # print(data.GrLivArea.shape)\n",
    "# # data.GrLivArea=oulierdetectfix(data.GrLivArea)\n",
    "# # data.TotalBsmtSF[data.TotalBsmtSF<upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for outliers on train data\n",
    "#  from scipy.stats import iqr\n",
    "# def outlierremove(ser)\n",
    "\n",
    "# print(data.GrLivArea.shape)\n",
    "# data.TotalBsmtSF[data.TotalBsmtSF<upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logical fix missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for missing all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the regfeatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unused reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for skew detect and fix all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unused cat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the items in data logically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pfds",
   "language": "python",
   "name": "pfds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
