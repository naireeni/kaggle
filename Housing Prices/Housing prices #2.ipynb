{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my second attempt at Housing Prices. I got pretty much lost in the first one but I learnt quite a few things. Even though I had poor results, it was good for skillup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic libraries first\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loadint data\n",
    "data=pd.read_csv(\"../Housing Prices/train.csv\")\n",
    "testdata=pd.read_csv(\"../Housing Prices/test.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nas for plot- just the train data. not part of final solution\n",
    "#lets fill nans\n",
    "# data.isna().sum()\n",
    "cont_features=[]\n",
    "cat_features=[]\n",
    "data.drop('Id',axis=1,inplace=True)\n",
    "data.GarageType.fillna('None')\n",
    "for col in data.columns:\n",
    "    if data[col].dtype=='int64' or data[col].dtype=='float64':\n",
    "        cont_features.append(col)\n",
    "        data[col].fillna(data[col].median(),inplace=True)\n",
    "    else:\n",
    "        cat_features.append(col)\n",
    "        data[col].fillna(data[col].value_counts().idxmax(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots I used extensively for feature engineering, feature selection\n",
    "# a = sns.FacetGrid(data=data,col=cont_features)\n",
    "for col in cont_features:\n",
    "    fig,ax=plt.subplots(figsize=(5,5))\n",
    "    sns.regplot(x=col,y='SalePrice', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots I used extensively for feature engineering, feature selection\n",
    "for col in cat_features:\n",
    "#     fig,ax=plt.subplots(figsize=(5,5))\n",
    "    sns.catplot(x=col,y='SalePrice', data=data,kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map I used extensively for feature engineering, feature selection\n",
    "fig, ax=plt.subplots(figsize=(40,40))\n",
    "sns.heatmap(data.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I got these using visual analysis of data\n",
    "cont_features2=['SalePrice','LotFrontage','MasVnrArea', 'TotalBsmtSF', '1stFlrSF','2ndFlrSF', 'GrLivArea'] \n",
    "cat_features2= ['YearBuilt','YearRemodAdd','BsmtFullBath', 'BsmtHalfBath','FullBath','HalfBath','OverallQual','TotRmsAbvGrd', 'Fireplaces','MSZoning','Street','LotShape','LotConfig','Condition1',\n",
    "        'BldgType','Neighborhood','HouseStyle',\n",
    "        'BsmtFinType1','Electrical','FireplaceQu','GarageType','SaleType','SaleCondition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewdetfix(feat):\n",
    "    sk=feat.skew()\n",
    "    if sk>1 or sk<1:\n",
    "#         fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "#         feat.fillna(0)\n",
    "#         sns.distplot(feat, ax=ax1)\n",
    "        feat=np.log1p(feat)\n",
    "#         feat.fillna(0)\n",
    "#         sns.distplot(feat,ax=ax2,color=\"k\")\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import percentile\n",
    "# print(data.GrLivArea.shape)\n",
    "\n",
    "\n",
    "def outlierdetectfix(feat):\n",
    "#     print(feat)\n",
    "    q25,q75=percentile(feat,25),percentile(feat,75)\n",
    "    iqr=(q75-q25)\n",
    "    iqr = q75 - q25\n",
    "#     type(feat)\n",
    "    lower = q25 - (iqr*1.5)\n",
    "    upper = q75 + (iqr*1.5)\n",
    "#     print(upper)\n",
    "    feat.loc[feat>upper] = upper\n",
    "    return feat\n",
    "\n",
    "\n",
    "# data.GrLivArea=oulierdetectfix(data.GrLivArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/numpy/lib/function_base.py:4291: RuntimeWarning: Invalid value encountered in percentile\n",
      "  interpolation=interpolation)\n"
     ]
    }
   ],
   "source": [
    "for col in cont_features2:\n",
    "    data[col]=skewdetfix(data[col])\n",
    "    data[col]=outlierdetectfix(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 80)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testdata['SalePrice']=1\n",
    "y=data[['SalePrice']].copy()\n",
    "data.drop('SalePrice',axis=1,inplace=True)\n",
    "data2=pd.concat([data,testdata],axis=0)\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nas\n",
    "data2.GarageType.fillna('None')\n",
    "for col in data2.columns:\n",
    "    if data2[col].dtype=='int64' or data2[col].dtype=='float64':\n",
    "#         cont_features.append(col)\n",
    "        data2[col].fillna(data2[col].median(),inplace=True)\n",
    "    else:\n",
    "#         cat_features.append(col)\n",
    "        data2[col].fillna(data2[col].value_counts().idxmax(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop extra cols\n",
    "for col in data2.columns:\n",
    "#     print (col)\n",
    "    if col not in cont_features2 and col not in cat_features2:\n",
    "        data2.drop(col, axis=1, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "data2['Make']=data2[['YearBuilt','YearRemodAdd']].max(axis=1)\n",
    "data2.loc[ data2['Make'] <= 1950, 'Make'] = 1950\n",
    "data2.loc[(data2['Make'] > 1950) & (data2['Make']<= 1960), 'Make'] = 1960\n",
    "data2.loc[(data2['Make'] > 1960) & (data2['Make']<= 1970), 'Make'] = 1970\n",
    "data2.loc[(data2['Make'] > 1970) & (data2['Make']<= 1980), 'Make'] = 1980\n",
    "data2.loc[(data2['Make'] > 1980) & (data2['Make']<= 1990), 'Make'] = 1990\n",
    "data2.loc[(data2['Make'] > 1990) & (data2['Make']<= 2000), 'Make'] = 2000\n",
    "data2.loc[(data2['Make'] > 2000) & (data2['Make']<= 2010), 'Make'] = 2010\n",
    "data2.loc[ data2['Make'] > 2010, 'Make'] = 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data2.drop(['YearBuilt','YearRemodAdd'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['TotalFSF']=data2['1stFlrSF']+data2['2ndFlrSF']\n",
    "data2.drop(['1stFlrSF','2ndFlrSF'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['TotalBath']=data2['BsmtFullBath']+data2['BsmtHalfBath']+data2['FullBath']+data2['HalfBath']\n",
    "data2.drop(['BsmtFullBath', 'BsmtHalfBath','FullBath','HalfBath'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used plots to get an idea of the relationships and the variables that seem necessary. I fixed the outliers in training data before anything else so that I can combine data and then do missing values, feature engineering and encoding on all the data together. Since the data isnt neccesarily ordinal, I will use onehot encode and then do PCA of the whole set together. That way I can try out different models without worrying about cat features. I will do a correlation check before to check if I am taking the right features. I might do another corr after encoding. lets see how this goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.LotShape.replace(['IR2','IR3'], 'other', inplace=True)\n",
    "data2.Condition1.replace(['Feedr','Artery','RRAn','PosN','RRAe','PosA','RRNn','RRNe'],'other',inplace=True)\n",
    "\n",
    "data2.Electrical.replace(['FuseA','FuseF','FuseP'],'other',inplace=True)\n",
    "\n",
    "data2.GarageType.replace(['BuiltIn','Basment','CarPort','2Types'],'other',inplace=True)\n",
    "data2.SaleType.replace(['COD','ConLD','ConLw','ConLi','CWD','Oth','Con'],'other',inplace=True)\n",
    "\n",
    "data2.SaleCondition.replace(['Family','Alloca','AdjLand'],'other',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#encoding and scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.drop('Neighborhood',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cont_features3=['LotFrontage', 'MasVnrArea', 'TotalBsmtSF', 'TotalFSF', 'GrLivArea'] \n",
    "cat_features3=[ 'Make','TotalBath','OverallQual','TotRmsAbvGrd', 'Fireplaces','MSZoning','Street','LotShape','LotConfig','Condition1',\n",
    "        'BldgType','HouseStyle',\n",
    "        'BsmtFinType1','Electrical','FireplaceQu','GarageType','SaleType','SaleCondition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(var):\n",
    "    return var.astype(\"category\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_features3:\n",
    "    data2[col]=encoder(data2[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data2[cat_features3])\n",
    "# dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2.drop(cat_features3,axis=1,inplace=True)\n",
    "data2=pd.concat([data2,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.drop(cat_features3,axis=1,inplace=True)\n",
    "# data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalFSF</th>\n",
       "      <th>Make_1950</th>\n",
       "      <th>Make_1960</th>\n",
       "      <th>Make_1970</th>\n",
       "      <th>Make_1980</th>\n",
       "      <th>Make_1990</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>GarageType_other</th>\n",
       "      <th>SaleType_ConLI</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_other</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.189655</td>\n",
       "      <td>5.283204</td>\n",
       "      <td>6.753438</td>\n",
       "      <td>7.444833</td>\n",
       "      <td>13.504539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.394449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.141245</td>\n",
       "      <td>7.141245</td>\n",
       "      <td>7.141245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.234107</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>6.825460</td>\n",
       "      <td>7.488294</td>\n",
       "      <td>13.590499</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.110874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.629363</td>\n",
       "      <td>7.448916</td>\n",
       "      <td>13.498378</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.442651</td>\n",
       "      <td>5.860786</td>\n",
       "      <td>7.044033</td>\n",
       "      <td>7.695758</td>\n",
       "      <td>14.004381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LotFrontage  MasVnrArea  TotalBsmtSF  GrLivArea   TotalFSF  Make_1950  \\\n",
       "0     4.189655    5.283204     6.753438   7.444833  13.504539          0   \n",
       "1     4.394449    0.000000     7.141245   7.141245   7.141245          0   \n",
       "2     4.234107    5.093750     6.825460   7.488294  13.590499          0   \n",
       "3     4.110874    0.000000     6.629363   7.448916  13.498378          0   \n",
       "4     4.442651    5.860786     7.044033   7.695758  14.004381          0   \n",
       "\n",
       "   Make_1960  Make_1970  Make_1980  Make_1990         ...           \\\n",
       "0          0          0          0          0         ...            \n",
       "1          0          0          1          0         ...            \n",
       "2          0          0          0          0         ...            \n",
       "3          0          1          0          0         ...            \n",
       "4          0          0          0          0         ...            \n",
       "\n",
       "   GarageType_Detchd  GarageType_other  SaleType_ConLI  SaleType_New  \\\n",
       "0                  0                 0               0             0   \n",
       "1                  0                 0               0             0   \n",
       "2                  0                 0               0             0   \n",
       "3                  1                 0               0             0   \n",
       "4                  0                 0               0             0   \n",
       "\n",
       "   SaleType_WD  SaleType_other  SaleCondition_Abnorml  SaleCondition_Normal  \\\n",
       "0            1               0                      0                     1   \n",
       "1            1               0                      0                     1   \n",
       "2            1               0                      0                     1   \n",
       "3            1               0                      1                     0   \n",
       "4            1               0                      0                     1   \n",
       "\n",
       "   SaleCondition_Partial  SaleCondition_other  \n",
       "0                      0                    0  \n",
       "1                      0                    0  \n",
       "2                      0                    0  \n",
       "3                      0                    0  \n",
       "4                      0                    0  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data2.iloc[:1460,:]\n",
    "test=data2.iloc[1460 : ,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will implement a min max scaler within bounds 0,1 so as to try to make the most sense interms of models where categorical and cont features dont go together and in general to apply PCA. trained on training data, transforming both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Instantiate encoder/scaler\n",
    "scaler = MinMaxScaler([0,1])\n",
    "scaler.fit(train[cont_features3]) \n",
    "train[cont_features3]=scaler.transform(train[cont_features3]) \n",
    "test[cont_features3]=scaler.transform(test[cont_features3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first step in reducing components via PCA is to find the interval where explained variance is between 96-97 percent. I will use a high number of components for PCA and then use the method \"exlained varience ratio available in SK learn PCA. It outputs fractions which I have rounded and converted into percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.93 19.34 25.79 30.71 34.97 38.66 41.91 45.03 48.06 50.97 53.75 56.38\n",
      " 58.95 61.28 63.46 65.49 67.41 69.15 70.84 72.35 73.83 75.21 76.58 77.86\n",
      " 79.07 80.26 81.38 82.47 83.51 84.48 85.43 86.34 87.2  88.04 88.85 89.6\n",
      " 90.34 91.04 91.72 92.36 92.94 93.48 94.   94.47 94.92 95.34 95.75 96.15\n",
      " 96.53 96.87 97.17 97.45 97.7  97.94 98.14 98.33 98.5  98.66 98.81 98.96]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "# X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=60)\n",
    "train2=pca.fit_transform(train)\n",
    "\n",
    "\n",
    "var=pca.explained_variance_ratio_\n",
    "var1=np.cumsum(np.round(var,decimals=4)*100)\n",
    "\n",
    "print(var1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "train_PCA=pca.fit(train)\n",
    "train_PCA=pca.transform(train)\n",
    "test_PCA=pca.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y.iloc[:1460,:]\n",
    "y_test=y.iloc[1460 : ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_trn,y_tst=train_test_split(train_PCA,y_train,test_size=0.33,random_state=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model I tried is Random Forest Regression. Since I did a lot of preprocessing, I wanted to have a taste of how the model performs. I expect ensembles to work the best for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomforest test\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf=RandomForestRegressor(n_estimators= 3000,min_samples_split= 5,min_samples_leaf= 2)\n",
    "\n",
    "# rf2.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/environments/PFDS/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=2, min_samples_split=5,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=3000, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(x_train,y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got an accuracy of 72 percent with manualy selected parameters. I will now try to use Sklearns Randomized CV search or grid searcg CV to do cross validation based parameter optimisation. Since my machine crashes, I will try to use Ms Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.720906411057179"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(x_test,y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_columns_idx = [data2.columns.get_loc(col) \n",
    "#                    for col in cat_features3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe    = OneHotEncoder(categorical_features=cat_columns_idx,sparse=False,handle_unknown='ignore',dtype='int')\n",
    "# # #  for col in cat_features3:\n",
    "# dummies=    ohe.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# lb = LabelBinarizer()\n",
    "# for col in cat_features3:\n",
    "#     feature=lb.fit_transform(data2[col])\n",
    "#     n_feature=pd.concat([n_feature,pd.DataFrame(feature)\n",
    "# #     data3=pd.concat([data2,n_feature],axis=1)\n",
    "#     print(n_feature.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[cat_features3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumm=pd.DataFrame(dummies)\n",
    "\n",
    "dumm.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def skewdetfix(feat):\n",
    "#     sk=feat.skew()\n",
    "#     if sk>1 or sk<1:\n",
    "#         feat=np.log(feat)\n",
    "#     return feat\n",
    "# # data.SalePrice.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import skew\n",
    "# skew(data.SalePrice)\n",
    "# data.SalePrice=np.log(data.SalePrice)\n",
    "# data.SalePrice.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just checking relation between garrage building and others \n",
    "# sns.relplot(x=\"GarageYrBlt\",y=\"YearRemodAdd\",data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #function for outliers on train data\n",
    "# #  from scipy.stats import iqr\n",
    "# # def outlierremove(ser)\n",
    "# from numpy import percentile\n",
    "# # print(data.GrLivArea.shape)\n",
    "\n",
    "\n",
    "# def oulierdetectfix(feat):\n",
    "# #     print(feat)\n",
    "#     q25,q75=percentile(feat,25),percentile(feat,75)\n",
    "#     iqr=(q75-q25)\n",
    "#     iqr = q75 - q25\n",
    "# #     type(feat)\n",
    "#     lower = q25 - (iqr*3)\n",
    "#     upper = q75 + (iqr*3)\n",
    "# #     print(upper)\n",
    "#     feat.loc[feat>upper] = upper\n",
    "# #     return feata\n",
    "# data.GrLivArea=oulierdetectfix(data.GrLivArea)\n",
    "# # print(data.GrLivArea.shape)\n",
    "# # data.GrLivArea=oulierdetectfix(data.GrLivArea)\n",
    "# # data.TotalBsmtSF[data.TotalBsmtSF<upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for outliers on train data\n",
    "#  from scipy.stats import iqr\n",
    "# def outlierremove(ser)\n",
    "\n",
    "# print(data.GrLivArea.shape)\n",
    "# data.TotalBsmtSF[data.TotalBsmtSF<upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logical fix missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for missing all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the regfeatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unused reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for skew detect and fix all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unused cat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the items in data logically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pfds",
   "language": "python",
   "name": "pfds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
